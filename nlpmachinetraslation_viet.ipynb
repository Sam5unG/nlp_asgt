{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "nlpmachinetraslation_viet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asYR_V_-GNIG",
        "outputId": "cf76082d-62f0-4ba5-b773-104d2be80e92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA1BkJQj_YR9",
        "outputId": "4cc7ee8d-e7be-4495-ae1c-1c0834847248"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 20  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 100000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "train_en = '/content/drive/MyDrive/Datasets from web/train.en'\n",
        "\n",
        "train_vi = '/content/drive/MyDrive/Datasets from web/train.vi'\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "#input_texts = []\n",
        "#target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(train_en, 'r', encoding='utf-8') as f:\n",
        "    ip_lines = f.read().split('\\n')\n",
        "\n",
        "with open(train_en, 'r', encoding='utf-8') as f:\n",
        "    op_lines = f.read().split('\\n')\n",
        "\n",
        "len(ip_lines), len(op_lines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(133318, 133318)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbVH4VAP_YSy",
        "outputId": "0e59cfb2-0c12-4925-e3d7-7eb1fef92cb8"
      },
      "source": [
        "considered_lines_index=[]\n",
        "considered_ip_lines=[]\n",
        "for i, line in enumerate(ip_lines[: min(num_samples, len(ip_lines) - 1)]):\n",
        "    if len(line) > 70:\n",
        "      pass\n",
        "    else:\n",
        "      considered_ip_lines.append(line)\n",
        "      considered_lines_index.append(i)\n",
        "    #input_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    #target_text = '\\t' + target_text + '\\n'\n",
        "    #input_texts.append(input_text)\n",
        "    #target_texts.append(target_text)\n",
        "      for char in line:\n",
        "          if char not in input_characters:\n",
        "\n",
        "              input_characters.add(char)\n",
        "op_lines_use = []\n",
        "for j in considered_lines_index:\n",
        "  op_lines_use.append(op_lines[j])\n",
        "\n",
        "for line in op_lines_use:     \n",
        "    for char in line:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in considered_ip_lines])\n",
        "max_decoder_seq_length = max([len(txt) for txt in op_lines_use])\n",
        "\n",
        "print('Number of samples:', min(num_samples, len(op_lines_use)))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "input_characters, len(op_lines_use), len(considered_ip_lines)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 42942\n",
            "Number of unique input tokens: 96\n",
            "Number of unique output tokens: 96\n",
            "Max sequence length for inputs: 70\n",
            "Max sequence length for outputs: 70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([' ',\n",
              "  '!',\n",
              "  '#',\n",
              "  '$',\n",
              "  '&',\n",
              "  '(',\n",
              "  '+',\n",
              "  ',',\n",
              "  '-',\n",
              "  '.',\n",
              "  '/',\n",
              "  '0',\n",
              "  '1',\n",
              "  '2',\n",
              "  '3',\n",
              "  '4',\n",
              "  '5',\n",
              "  '6',\n",
              "  '7',\n",
              "  '8',\n",
              "  '9',\n",
              "  ':',\n",
              "  ';',\n",
              "  '=',\n",
              "  '?',\n",
              "  'A',\n",
              "  'B',\n",
              "  'C',\n",
              "  'D',\n",
              "  'E',\n",
              "  'F',\n",
              "  'G',\n",
              "  'H',\n",
              "  'I',\n",
              "  'J',\n",
              "  'K',\n",
              "  'L',\n",
              "  'M',\n",
              "  'N',\n",
              "  'O',\n",
              "  'P',\n",
              "  'Q',\n",
              "  'R',\n",
              "  'S',\n",
              "  'T',\n",
              "  'U',\n",
              "  'V',\n",
              "  'W',\n",
              "  'X',\n",
              "  'Y',\n",
              "  'Z',\n",
              "  '_',\n",
              "  'a',\n",
              "  'b',\n",
              "  'c',\n",
              "  'd',\n",
              "  'e',\n",
              "  'f',\n",
              "  'g',\n",
              "  'h',\n",
              "  'i',\n",
              "  'j',\n",
              "  'k',\n",
              "  'l',\n",
              "  'm',\n",
              "  'n',\n",
              "  'o',\n",
              "  'p',\n",
              "  'q',\n",
              "  'r',\n",
              "  's',\n",
              "  't',\n",
              "  'u',\n",
              "  'v',\n",
              "  'w',\n",
              "  'x',\n",
              "  'y',\n",
              "  'z',\n",
              "  'á',\n",
              "  'ã',\n",
              "  'è',\n",
              "  'é',\n",
              "  'í',\n",
              "  'ñ',\n",
              "  'ó',\n",
              "  'ö',\n",
              "  'ø',\n",
              "  'ü',\n",
              "  'ā',\n",
              "  'ย',\n",
              "  'ร',\n",
              "  'อ',\n",
              "  '่',\n",
              "  '–',\n",
              "  '—',\n",
              "  '…'],\n",
              " 42942,\n",
              " 42942)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrVykr2o_YTN",
        "outputId": "05f03631-776b-4ca7-dec7-f9e4b7722695"
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "input_token_index\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '!': 1,\n",
              " '#': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " '(': 5,\n",
              " '+': 6,\n",
              " ',': 7,\n",
              " '-': 8,\n",
              " '.': 9,\n",
              " '/': 10,\n",
              " '0': 11,\n",
              " '1': 12,\n",
              " '2': 13,\n",
              " '3': 14,\n",
              " '4': 15,\n",
              " '5': 16,\n",
              " '6': 17,\n",
              " '7': 18,\n",
              " '8': 19,\n",
              " '9': 20,\n",
              " ':': 21,\n",
              " ';': 22,\n",
              " '=': 23,\n",
              " '?': 24,\n",
              " 'A': 25,\n",
              " 'B': 26,\n",
              " 'C': 27,\n",
              " 'D': 28,\n",
              " 'E': 29,\n",
              " 'F': 30,\n",
              " 'G': 31,\n",
              " 'H': 32,\n",
              " 'I': 33,\n",
              " 'J': 34,\n",
              " 'K': 35,\n",
              " 'L': 36,\n",
              " 'M': 37,\n",
              " 'N': 38,\n",
              " 'O': 39,\n",
              " 'P': 40,\n",
              " 'Q': 41,\n",
              " 'R': 42,\n",
              " 'S': 43,\n",
              " 'T': 44,\n",
              " 'U': 45,\n",
              " 'V': 46,\n",
              " 'W': 47,\n",
              " 'X': 48,\n",
              " 'Y': 49,\n",
              " 'Z': 50,\n",
              " '_': 51,\n",
              " 'a': 52,\n",
              " 'b': 53,\n",
              " 'c': 54,\n",
              " 'd': 55,\n",
              " 'e': 56,\n",
              " 'f': 57,\n",
              " 'g': 58,\n",
              " 'h': 59,\n",
              " 'i': 60,\n",
              " 'j': 61,\n",
              " 'k': 62,\n",
              " 'l': 63,\n",
              " 'm': 64,\n",
              " 'n': 65,\n",
              " 'o': 66,\n",
              " 'p': 67,\n",
              " 'q': 68,\n",
              " 'r': 69,\n",
              " 's': 70,\n",
              " 't': 71,\n",
              " 'u': 72,\n",
              " 'v': 73,\n",
              " 'w': 74,\n",
              " 'x': 75,\n",
              " 'y': 76,\n",
              " 'z': 77,\n",
              " 'á': 78,\n",
              " 'ã': 79,\n",
              " 'è': 80,\n",
              " 'é': 81,\n",
              " 'í': 82,\n",
              " 'ñ': 83,\n",
              " 'ó': 84,\n",
              " 'ö': 85,\n",
              " 'ø': 86,\n",
              " 'ü': 87,\n",
              " 'ā': 88,\n",
              " 'ย': 89,\n",
              " 'ร': 90,\n",
              " 'อ': 91,\n",
              " '่': 92,\n",
              " '–': 93,\n",
              " '—': 94,\n",
              " '…': 95}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylg0PsV1FC5r"
      },
      "source": [
        "input_texts = considered_ip_lines\n",
        "target_texts = op_lines_use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L59BR08Fcvg",
        "outputId": "a073291e-1459-4813-f745-3f8f2ca37103"
      },
      "source": [
        "len(target_texts), len(input_texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42942, 42942)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_i5OQvY_YTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1de9d7-99f1-4cbf-bee9-2f3d3b51f839"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "encoder_input_data.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42942, 70, 96)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcrAEd-T_YTe",
        "outputId": "a2e60f1a-7d50-480f-a697-0c03d682983a"
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
        "    \n",
        "encoder_input_data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNnEkaR7_YTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf09288-1714-45e1-8a24-0257c9b0dd3e"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "537/537 [==============================] - 13s 19ms/step - loss: 1.9834 - accuracy: 0.5175 - val_loss: 1.2732 - val_accuracy: 0.6405\n",
            "Epoch 2/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 1.2207 - accuracy: 0.6535 - val_loss: 1.0768 - val_accuracy: 0.6917\n",
            "Epoch 3/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 1.0446 - accuracy: 0.7006 - val_loss: 0.9614 - val_accuracy: 0.7212\n",
            "Epoch 4/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.9446 - accuracy: 0.7276 - val_loss: 0.8994 - val_accuracy: 0.7398\n",
            "Epoch 5/20\n",
            "537/537 [==============================] - 10s 19ms/step - loss: 0.8774 - accuracy: 0.7458 - val_loss: 0.8373 - val_accuracy: 0.7568\n",
            "Epoch 6/20\n",
            "537/537 [==============================] - 10s 19ms/step - loss: 0.8219 - accuracy: 0.7612 - val_loss: 0.8083 - val_accuracy: 0.7647\n",
            "Epoch 7/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.7841 - accuracy: 0.7710 - val_loss: 0.7733 - val_accuracy: 0.7742\n",
            "Epoch 8/20\n",
            "537/537 [==============================] - 10s 19ms/step - loss: 0.7498 - accuracy: 0.7812 - val_loss: 0.7529 - val_accuracy: 0.7808\n",
            "Epoch 9/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.7269 - accuracy: 0.7874 - val_loss: 0.7314 - val_accuracy: 0.7865\n",
            "Epoch 10/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.7026 - accuracy: 0.7943 - val_loss: 0.7228 - val_accuracy: 0.7896\n",
            "Epoch 11/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.6788 - accuracy: 0.8007 - val_loss: 0.6933 - val_accuracy: 0.7971\n",
            "Epoch 12/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.6594 - accuracy: 0.8063 - val_loss: 0.6765 - val_accuracy: 0.8023\n",
            "Epoch 13/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.6408 - accuracy: 0.8112 - val_loss: 0.6624 - val_accuracy: 0.8065\n",
            "Epoch 14/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.6246 - accuracy: 0.8162 - val_loss: 0.6532 - val_accuracy: 0.8088\n",
            "Epoch 15/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.6071 - accuracy: 0.8209 - val_loss: 0.6502 - val_accuracy: 0.8106\n",
            "Epoch 16/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.5896 - accuracy: 0.8260 - val_loss: 0.6258 - val_accuracy: 0.8167\n",
            "Epoch 17/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.5750 - accuracy: 0.8298 - val_loss: 0.6105 - val_accuracy: 0.8214\n",
            "Epoch 18/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.5637 - accuracy: 0.8331 - val_loss: 0.6052 - val_accuracy: 0.8231\n",
            "Epoch 19/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.5460 - accuracy: 0.8380 - val_loss: 0.5874 - val_accuracy: 0.8287\n",
            "Epoch 20/20\n",
            "537/537 [==============================] - 10s 18ms/step - loss: 0.5341 - accuracy: 0.8418 - val_loss: 0.5779 - val_accuracy: 0.8313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tibvmstpSN-o",
        "outputId": "a82c3688-35f9-4357-e5fa-ddc6b34e7218"
      },
      "source": [
        "\n",
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    #target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Rachel Pike : The science behind a climate headline\n",
            "Decoded sentence: avin : This is a problem with an example of these                      \n",
            "-\n",
            "Input sentence: They are both two branches of the same field of atmospheric science .\n",
            "Decoded sentence: o are yount at the problem that we can do this in the companies .      \n",
            "-\n",
            "Input sentence: That report was written by 620 scientists from 40 countries .\n",
            "Decoded sentence: hate responsible , it comes in the world and so income to .            \n",
            "-\n",
            "Input sentence: They wrote almost a thousand pages on the topic .\n",
            "Decoded sentence: o , they are all the way that they were there .                        \n",
            "-\n",
            "Input sentence: Over 15,000 scientists go to San Francisco every year for that .\n",
            "Decoded sentence: ever , maybe we have the way they were the way they come back .        \n",
            "-\n",
            "Input sentence: We blow it up and look at the pieces .\n",
            "Decoded sentence: e diff like a lot of things are there .                                \n",
            "-\n",
            "Input sentence: This is the EUPHORE Smog Chamber in Spain .\n",
            "Decoded sentence: his is the way , the answer is a partic .                              \n",
            "-\n",
            "Input sentence: But still , we look at the pieces .\n",
            "Decoded sentence: ose till , they are not attings .                                      \n",
            "-\n",
            "Input sentence: And it takes weeks to perform our integrations .\n",
            "Decoded sentence: e it is that model of the state of the commen .                        \n",
            "-\n",
            "Input sentence: We also fly all over the world looking for this thing .\n",
            "Decoded sentence: o also hand of the way they have to be about this wor .                \n",
            "-\n",
            "Input sentence: I recently joined a field campaign in Malaysia . There are others .\n",
            "Decoded sentence: er celled an interesting and a half and the way we did it lives .      \n",
            "-\n",
            "Input sentence: This is the tower in the middle of the rainforest , from above .\n",
            "Decoded sentence: it is the story of the big change that we &apos;re all about .         \n",
            "-\n",
            "Input sentence: And this is the tower from below .\n",
            "Decoded sentence: o this is the world of complex .                                       \n",
            "-\n",
            "Input sentence: So maybe you took a similar aircraft to get here today .\n",
            "Decoded sentence: o maybe you have the answer in the way that we could .                 \n",
            "-\n",
            "Input sentence: We hire military and test pilots to do the maneuvering .\n",
            "Decoded sentence: e histeriate actually is the same thing to do this one .               \n",
            "-\n",
            "Input sentence: We have to get special flight clearance .\n",
            "Decoded sentence: e have to be starting the change care .                                \n",
            "-\n",
            "Input sentence: We do all of this to understand the chemistry of one molecule .\n",
            "Decoded sentence: o dead it hoose to the solition of the computer companies ...          \n",
            "-\n",
            "Input sentence: So you can imagine the scale of the effort .\n",
            "Decoded sentence: o you can also come to the thing the come .                            \n",
            "-\n",
            "Input sentence: Thank you very much .\n",
            "Decoded sentence: hank you very much .                                                   \n",
            "-\n",
            "Input sentence: Christopher deCharms : A look inside the brain in real time\n",
            "Decoded sentence: hirds of the art , the same thing is the same thing in 1970 .          \n",
            "-\n",
            "Input sentence: You can mimic what you can see .\n",
            "Decoded sentence: o can &apos;t see any change .                                         \n",
            "-\n",
            "Input sentence: You can program the hundreds of muscles in your arm .\n",
            "Decoded sentence: o can be the power of the bottom of a completely .                     \n",
            "-\n",
            "Input sentence: I &apos;m going to tell you about that technology .\n",
            "Decoded sentence:  &apos;m not going to show you an example there .                      \n",
            "-\n",
            "Input sentence: People envision this as being very difficult .\n",
            "Decoded sentence: eep of the idea of was in the carches liver .                          \n",
            "-\n",
            "Input sentence: It was terribly dangerous .\n",
            "Decoded sentence: o traie the car called St .                                            \n",
            "-\n",
            "Input sentence: You could be attacked by white blood cells in the arteries .\n",
            "Decoded sentence: o could not actually do that in the way they are preside .             \n",
            "-\n",
            "Input sentence: But now , we have a real technology to do this .\n",
            "Decoded sentence: now how I was a lot of things that we do today .                       \n",
            "-\n",
            "Input sentence: We &apos;re going to fly into my colleague Peter &apos;s brain .\n",
            "Decoded sentence: o &apos;s going to the control is a way to do a lot of complex .       \n",
            "-\n",
            "Input sentence: We &apos;re going to do it non-invasively using MRI .\n",
            "Decoded sentence: o &apos;s going to be able to do it in a construct .                   \n",
            "-\n",
            "Input sentence: He can look at these 65,000 points of activation per second .\n",
            "Decoded sentence: e can they have the way , the same thing is coming for me .            \n",
            "-\n",
            "Input sentence: This is a fourth alternative that you are soon going to have .\n",
            "Decoded sentence: his is a lot of an interesting and they have to be a thousan .         \n",
            "-\n",
            "Input sentence: So put your arms back up and flex your bicep .\n",
            "Decoded sentence: o put you an example of a sense of a context .                         \n",
            "-\n",
            "Input sentence: When they control their brain , they can control their pain .\n",
            "Decoded sentence: hen they come to the world , the answer is the same proble .           \n",
            "-\n",
            "Input sentence: I &apos;ve seen inside my brain . You will too , soon .\n",
            "Decoded sentence:  : design the company was a bunch of a son work in .                   \n",
            "-\n",
            "Input sentence: When you do , what do you want to control ?\n",
            "Decoded sentence: o hoy , moye , what do you do that books ?                             \n",
            "-\n",
            "Input sentence: But I want to leave with you the big question .\n",
            "Decoded sentence: u he a wonderful thing is the world to me .                            \n",
            "-\n",
            "Input sentence: Where will we take it ?\n",
            "Decoded sentence: ot which is the idea ?                                                 \n",
            "-\n",
            "Input sentence: Beeban Kidron : The shared wonder of film\n",
            "Decoded sentence: enable a sense of a second project is 1.                               \n",
            "-\n",
            "Input sentence: We are inveterate storytellers .\n",
            "Decoded sentence: ear in this complex states on .                                        \n",
            "-\n",
            "Input sentence: Indeed , it is hard to find a subject that film has yet to tackle .\n",
            "Decoded sentence: one did it was a little bit of the same thing that we do the same .    \n",
            "-\n",
            "Input sentence: As a filmmaker , it worried me .\n",
            "Decoded sentence: as is a more and shape of it .                                         \n",
            "-\n",
            "Input sentence: As a human being , it puts the fear of God in me .\n",
            "Decoded sentence: at a my sound , they were the way they do this .                       \n",
            "-\n",
            "Input sentence: The films were curated and contextualized .\n",
            "Decoded sentence: o field the are what we can do the begin .                             \n",
            "-\n",
            "Input sentence: The outcome , immediate .\n",
            "Decoded sentence: o out of manage , it .                                                 \n",
            "-\n",
            "Input sentence: It was an education of the most profound and transformative kind .\n",
            "Decoded sentence: o take a model that is the state of the back to a complete life .      \n",
            "-\n",
            "Input sentence: It &apos;s a remarkable comment on slums , poverty and aspiration .\n",
            "Decoded sentence: t &apos;s a practical thing that I &apos;m a lot of sense of its .     \n",
            "-\n",
            "Input sentence: Frank Capra &apos;s classic values independence and propriety .\n",
            "Decoded sentence: nan Staria : The more and the answer is an example of the bear .       \n",
            "-\n",
            "Input sentence: It shows how to do right , how to be heroically awkward .\n",
            "Decoded sentence: oss this one the world &apos;s a personal to the back .                \n",
            "-\n",
            "Input sentence: After all , Jimmy Stewart filibustered for two entire reels .\n",
            "Decoded sentence: etter , a little bit of the same thing that come from come .           \n",
            "-\n",
            "Input sentence: As they watch more films their lives got palpably richer .\n",
            "Decoded sentence: st he would be able to see the answer in the came and ...              \n",
            "-\n",
            "Input sentence: &quot; To Sir , with Love &quot; ignited its teen audience .\n",
            "Decoded sentence: o quite we &apos;ve all about the way , the same problem .             \n",
            "-\n",
            "Input sentence: And those without friends started making them .\n",
            "Decoded sentence: o tow with the same thing is the same thing .                          \n",
            "-\n",
            "Input sentence: The films provided communality across all manner of divide .\n",
            "Decoded sentence: o five people is that it is a lot of sense of an experience .          \n",
            "-\n",
            "Input sentence: And the stories they held provided a shared experience .\n",
            "Decoded sentence: o the stories that he did some all as the consequenc .                 \n",
            "-\n",
            "Input sentence: Who was right , who wrong ?\n",
            "Decoded sentence: or was it , what do you ?                                              \n",
            "-\n",
            "Input sentence: What would they do under the same conditions ?\n",
            "Decoded sentence: hat would they come to the self-design there ?                         \n",
            "-\n",
            "Input sentence: Was the tale told well ?\n",
            "Decoded sentence: s the man that we do ?                                                 \n",
            "-\n",
            "Input sentence: Was there a hidden message ?\n",
            "Decoded sentence: att there a simple issues ?                                            \n",
            "-\n",
            "Input sentence: How has the world changed ? How could it be different ?\n",
            "Decoded sentence: ow , that would be a lot of different ? Well the begi .                \n",
            "-\n",
            "Input sentence: And they themselves had not known they cared .\n",
            "Decoded sentence: o they text me that we can do that possible .                          \n",
            "-\n",
            "Input sentence: I have an aunt who is a wonderful storyteller .\n",
            "Decoded sentence: h have a name that I was been doing my front .                         \n",
            "-\n",
            "Input sentence: I was past 40 when my father died .\n",
            "Decoded sentence: e &apos;s personal , the same that .                                   \n",
            "-\n",
            "Input sentence: He never mentioned that journey .\n",
            "Decoded sentence: e need to make the bottom ago .                                        \n",
            "-\n",
            "Input sentence: After two years in hiding , my grandfather appeared in London .\n",
            "Decoded sentence: ffet the man was the animal way to do a little bit of anybody .        \n",
            "-\n",
            "Input sentence: He was never right again .\n",
            "Decoded sentence: e wateve because attain .                                              \n",
            "-\n",
            "Input sentence: And his story was hushed as he assimilated .\n",
            "Decoded sentence: ow is thousands , and they are not easier .                            \n",
            "-\n",
            "Input sentence: But within the reels lie purpose and meaning .\n",
            "Decoded sentence: ow with this thing that we are not even bee .                          \n",
            "-\n",
            "Input sentence: We honor reading , why not honor watching with the same passion ?\n",
            "Decoded sentence: o hore he does a lot of different , the same thing is the same .       \n",
            "-\n",
            "Input sentence: Consider &quot; Citizen Kane &quot; as valuable as Jane Austen .\n",
            "Decoded sentence: opide &apos;s a personal and a hand of the way , it comes it .         \n",
            "-\n",
            "Input sentence: They are neither feral nor myopically self-absorbed .\n",
            "Decoded sentence: o are than the beginning of the beautiful complex .                    \n",
            "-\n",
            "Input sentence: Thank you .\n",
            "Decoded sentence: hank you .                                                             \n",
            "-\n",
            "Input sentence: Ellen Jorgensen : Biohacking -- you can do it , too\n",
            "Decoded sentence: llow robot in a different , they &apos;re a good .                     \n",
            "-\n",
            "Input sentence: It &apos;s a great time to be a molecular biologist .\n",
            "Decoded sentence: t &apos;s a child project in the way and see there .                   \n",
            "-\n",
            "Input sentence: Reading and writing DNA code is getting easier and cheaper .\n",
            "Decoded sentence: eading and the are was an interesting in the way seconds .             \n",
            "-\n",
            "Input sentence: So who gets to do it ?\n",
            "Decoded sentence: o who esto out of it ?                                                 \n",
            "-\n",
            "Input sentence: I think we &apos;d all be pretty comfortable with this guy doing it .\n",
            "Decoded sentence: et it would &apos;t be a lot of sense of the animal things there .     \n",
            "-\n",
            "Input sentence: But what about that guy ?\n",
            "Decoded sentence: o what about that way ?                                                \n",
            "-\n",
            "Input sentence: In 2009 , I first heard about DIYbio .\n",
            "Decoded sentence: ow , I always wanted a lot of are ...                                  \n",
            "-\n",
            "Input sentence: I thought so .\n",
            "Decoded sentence: he thougho .                                                           \n",
            "-\n",
            "Input sentence: The press started calling us .\n",
            "Decoded sentence: o pers start in the comment .                                          \n",
            "-\n",
            "Input sentence: But now , three years later , here &apos;s where we stand .\n",
            "Decoded sentence: now they had the world , they &apos;re a lot of sensing .              \n",
            "-\n",
            "Input sentence: Let me take you on a little tour .\n",
            "Decoded sentence: e met my thing about the world .                                       \n",
            "-\n",
            "Input sentence: Biohackers work alone .\n",
            "Decoded sentence: in creation , they do .                                                \n",
            "-\n",
            "Input sentence: We work in groups , in big cities — — and in small villages .\n",
            "Decoded sentence: o would high in a star in the way , the answer is science .            \n",
            "-\n",
            "Input sentence: We reverse engineer lab equipment .\n",
            "Decoded sentence: e reers the best information does .                                    \n",
            "-\n",
            "Input sentence: We genetically engineer bacteria .\n",
            "Decoded sentence: o everything is the answer inder .                                     \n",
            "-\n",
            "Input sentence: We like to build things .\n",
            "Decoded sentence: o like to be a things .                                                \n",
            "-\n",
            "Input sentence: Then we like to take things apart .\n",
            "Decoded sentence: ow when it is one that we are it .                                     \n",
            "-\n",
            "Input sentence: We make things grow .\n",
            "Decoded sentence: o make it to be in .                                                   \n",
            "-\n",
            "Input sentence: We make things glow .\n",
            "Decoded sentence: o make it to be it .                                                   \n",
            "-\n",
            "Input sentence: And we make cells dance .\n",
            "Decoded sentence: o me make it all off .                                                 \n",
            "-\n",
            "Input sentence: I &apos;m not going to minimize those concerns .\n",
            "Decoded sentence: hat &apos;s not the same thing completely use .                        \n",
            "-\n",
            "Input sentence: That &apos;s a lot more than conventional science has done .\n",
            "Decoded sentence: t &apos;m always the best in the companies and superator .             \n",
            "-\n",
            "Input sentence: Now , we follow state and local regulations .\n",
            "Decoded sentence: o , we feel the animal world and so only .                             \n",
            "-\n",
            "Input sentence: I &apos;ve actually chosen to take a different kind of risk .\n",
            "Decoded sentence:  : I am in my computer in the way that we are explain them .           \n",
            "-\n",
            "Input sentence: I signed up for something called the Personal Genome Project .\n",
            "Decoded sentence:  is becoming the story of the back and so on the companys .            \n",
            "-\n",
            "Input sentence: So this stuff is just beginning .\n",
            "Decoded sentence: o this story is the animal men .                                       \n",
            "-\n",
            "Input sentence: We &apos;re only seeing just the tip of the DNA iceberg .\n",
            "Decoded sentence: o &apos;t not look like the solvent of a single paint .                \n",
            "-\n",
            "Input sentence: Let me show you what you could do right now .\n",
            "Decoded sentence: o mes that we are not all of the world one .                           \n",
            "-\n",
            "Input sentence: I discovered an invasive species in my own backyard .\n",
            "Decoded sentence: e is deally an animal thing that we can do it from .                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyi-_sWV_YTy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}